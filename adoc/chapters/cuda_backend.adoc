// %%%%%%%%%%%%%%%%%%%%%%%%%%%% begin cuda_backend %%%%%%%%%%%%%%%%%%%%%%%%%%%%

[appendix]
[[chapter:cuda-backend]]
= CUDA backend specification

This chapter describes how the SYCL general programming model is mapped on top
of CUDA, and how the SYCL generic interoperability interface must be
implemented by vendors providing SYCL for CUDA implementations to ensure SYCL
applications written for the CUDA backend are interoperable.

The CUDA backend is enabled using the `sycl::backend::cuda` value of `enum
class backend`. That means that when the CUDA backend is active, the value of
`sycl::is_backend_active<sycl::backend::cuda>::value` will be `true`, and the
preprocessor macro `SYCL_BACKEND_CUDA` will be defined.

The CUDA backend requires an installation of CUDA SDK as well as one or more
CUDA devices available in the system.
[[sec:cuda:introduction]]
== Introduction

[[sec:cuda:mapping_of_sycl_programming_model]]
== Mapping of SYCL programming model

This section gives a general overview of how the SYCL programming model maps to
CUDA. These two programming models are pretty similar in essence however they do
have a few differences in terminology and architecture.

[[sub:cuda:platform_model]]
=== Platform Model

TODO: Platform

TODO: Device

A SYCL <<context>> simply maps to one, or multiple CUDA contexts. Indeed while
a CUDA context is tied to a single device, this is not the case for a SYCL
<<context>> and the CUDA backend implementation may use multiple CUDA contexts
to emulate a SYCL <<context>> containing multiple devices. Additionally, while
SYCL contexts are simple objects passed around either implicitly or explicitly,
CUDA contexts require to be activated on the current thread to be used by other
CUDA entry points. Therefore any use of the SYCL APIs with a CUDA backend may
modify the current active context on the thread, and no guarantee is provided
that any existing active CUDA context would be restored by SYCL.

A SYCL <<queue>> simply maps to one or multiple CUDA streams. Indeed while a
CUDA stream is in-order, a SYCL <<queue>> isn't, so a CUDA backend implementation
may use multiple CUDA streams to implement an out of order SYCL <<queue>>.

[[sub:cuda:memory_model]]
=== Memory model

==== Memory Allocations

When non-host accessors to buffers are created without [code]#target::host_buffer# they need to allocate memory for their contents on the device. For example using [code]#cudaMalloc3D()#, [code]#cudaMallocPitch()# or [code]#cudaMalloc()#.

When accessors to images are created without [code]#target::host_buffer# they allocate memory, for example using [code]#cudaMalloc3DArray()# or [code]#cudaMallocArray()#. 

When non-host accessors are created with [code]#target::host_buffer# they can, for example use [code]#cudaHostAlloc()# to allocate pinned memory on host.

Table <<table.cuda.memmodel.USM>> specifies which underlying CUDA functions can be used for USM allocations. For shared USM allocations this would mean memory is managed (moved between host and different devices) by CUDA runtime. Alternatively shared USM allocations can be managed by SYCL runtime, using non-managed CUDA allocation on device when needed, such as [code]#cudaMalloc()#.

[[table.cuda.memmodel.USM]]
.Cuda functions that could be used to allocate SYCL USM allocations
[width="100%",options="header",cols="50%,50%"]
|====
| SYCL USM type | CUDA function
| device | [code]#cudaMalloc()#
| host | [code]#cudaHostAlloc()#
| shared | [code]#cudaMallocManaged()#
|====

==== Samplers

In both SYCL and CUDA samplers consist of addressing mode, filtering mode and coordinate normalization mode. Mapping between SYCL and CUDA values is defined in tables <<table.cuda.memmodel.sampler_addressing>>, <<table.cuda.memmodel.sampler_filtering>> and <<table.cuda.memmodel.sampler_normalization>>. In CUDA addressing modes for all dimesnions will be the same, as CUDA allows different addressing modes for different dimesnions, while SYCL does not. 

[[table.cuda.memmodel.sampler_addressing]]
.Mapping of SYCL sampler addressing modes to CUDA
[width="100%",options="header",cols="50%,50%"]
|====
| SYCL sampler addressing mode | CUDA sampler addressing mode
| [code]#sycl::addressing_mode::mirrored_repeat# | [code]#cudaAddressModeMirror#
| [code]#sycl::addressing_mode::repeat# | [code]#cudaAddressModeWrap#
| [code]#sycl::addressing_mode::clamp_to_edge# | [code]#cudaAddressModeClamp#
| [code]#sycl::addressing_mode::clamp# | [code]#cudaAddressModeClamp#
| [code]#sycl::addressing_mode::none# | [code]#cudaAddressModeBorder#
|====

SYCL allows [code]#sycl::addressing_mode::mirrored_repeat# and [code]#sycl::addressing_mode::repeat# to be used together with unnormalized coordinates. In this case the resulting coordinates are undefined. CUDA does not allow this, so if [code]#sycl::addressing_mode::mirrored_repeat# or [code]#sycl::addressing_mode::repeat# is specified together with unnormalized coordinates, [code]#cudaAddressModeBorder# is used instead.

[[table.cuda.memmodel.sampler_filtering]]
.Mapping of SYCL sampler filtering modes to CUDA
[width="100%",options="header",cols="50%,50%"]
|====
| SYCL sampler filtering mode | CUDA sampler filtering mode
| [code]#sycl::filtering_mode::nearest# | [code]#cudaFilterModePoint#
| [code]#sycl::filtering_mode::linear# | [code]#cudaFilterModeLinear#
|====

[[table.cuda.memmodel.sampler_normalization]]
.Mapping of SYCL sampler coordinate normalization modes to CUDA
[width="100%",options="header",cols="50%,50%"]
|====
| SYCL sampler coordinate normalization mode | CUDA sampler coordinate normalization mode
| [code]#sycl::coordinate_normalization_mode::normalized# | [code]#normalizedCoords = true#
| [code]#sycl::coordinate_normalization_mode::unnormalized# | [code]#normalizedCoords = false#
|====

==== Address Spaces

Table <<table.cuda.memmodel.address_spaces>> maps SYCL address spaces to CUDA address spaces.

[[table.cuda.memmodel.address_spaces]]
.Mapping from SYCL address spaces to CUDA address spaces
[width="100%",options="header",cols="50%,50%"]
|====
| SYCL Address Space | CUDA Address Space
| Global memory | global
| Local memory | shared
| Private memory | registers or local
| Generic memory | generic
| Constant memory | const
|====

==== Atomics

Not all CUDA devices support all memory orders. If a particular memory order is unsupported by a CUDA device, it can be unsupported in the SYCL CUDA backend for that device. Sequentially consistent atomics are currently not supported on any device, so the SYCL CUDA backend is not required to implement them. The mappings of other memory orders (when supported by the device) is defined in table <<table.cuda.memmodel.memory_orders>>.

[[table.cuda.memmodel.memory_orders]]
.Mapping from [code]#sycl::memory_order# to PTX ISA memory orders
[width="100%",options="header",cols="50%,50%"]
|====
| [code]#sycl::memory_order# | PTX ISA Memory Order
| [code]#memory_order::relaxed# | relaxed
| [code]#memory_order::acquire# | acquire
| [code]#memory_order::release# | release
| [code]#memory_order::acq_rel# | acq_rel
| [code]#memory_order::seq_cst# | undefined
|====

Mapping of memory scopes (when supported by the device) is defined in table [table.cuda.memmodel.memory_scopes]. [code]#memory_scope::work_item# does not require any consistency between different work items, so it can be mapped to non-atomic operation.

[[table.cuda.memmodel.memory_scopes]]
.Mapping from [code]#sycl::memory_scope# to PTX ISA memory scopes
[width="100%",options="header",cols="50%,50%"]
|====
| [code]#sycl::memory_scope# | PTX ISA Memory Scope
| [code]#memory_scope::work_item# | 
| [code]#memory_scope::sub_group# | cta
| [code]#memory_scope::work_group# | cta
| [code]#memory_scope::device# | gpu
| [code]#memory_scope::system# | system
|====

==== Fences

If a device supports the [code]#fence# PTX instruction the mapping of memory orders is defined in <<table.cuda.memmodel.fence_memory_orders>>. Otherwise all memory orders (except relaxed) are mapped to the [code]#membar# instruction.

[[table.cuda.memmodel.fence_memory_orders]]
.Mapping from [code]#sycl::memory_order# to PTX ISA memory orders when used in fences
[width="100%",options="header",cols="50%,50%"]
|====
| [code]#sycl::memory_order# | PTX ISA Memory Order
| [code]#memory_order::relaxed# | none
| [code]#memory_order::acquire# | acq_rel
| [code]#memory_order::release# | acq_rel
| [code]#memory_order::acq_rel# | acq_rel
| [code]#memory_order::seq_cst# | sc
|====

If future versions of PTX ISA define fence instructions with only acquire or only release memory order, these can be used as well for [code]#memory_order::acquire# and [code]#memory_order::release# on devices that support them.

Mapping of SYCL memory scopes to PTX ISA is the same as for atomics. It is defined in <<table.cuda.memmodel.memory_scopes>>.

[[sub:cuda:execution_model]]
=== Execution Model

CUDA's execution model is similar to SYCL's. CUDA uses kernels to
offload computation, splitting the host and GPU into asynchronous 
computing devices. In general, except for CUDA's dynamic 
parallelism extensions, kernels are called by the host. One 
difference between CUDA and SYCL execution models is that CUDA 
uses Single Instruction Multiple Thread (SIMT) while SYCL uses 
Single Instruction Multiple Data (SIMD) kernels. SIMT kernels use 
multiple scalar instructions acting on non-contiguous data.  SIMD 
kernels use vector instructions acting on contiguous data. SIMT 
can be used in place of SIMD but not the other way around, as SIMD 
requires memory blocks to have no interruptions within the data, 
while SIMT does not have this as a requirement. 

CUDA GPUs are constructed out of streaming multiprocessors (SM) 
which perform the actual computation. Each SM consists of 8 scalar 
cores, shared memory, registers, a load/store unit, and a scheduler 
unit. CUDA uses a hierarchy of threads to organize the execution of
kernels. Kernels are split up into thread blocks. The threadblocks 
form a grid each thread can identify its location within the grid 
using a block ID. The grid is a concept used to index threadblocks 
the grid can be one, two, or three dimensions. Each thread block is 
tied to a single SM. Similar to a thread block's location within the 
grid, each thread's position within the block can be identified with 
a one, two, or three dimensional thread ID. 

Pre-Volta GPU architectures breaks thread blocks into warps which 
consist of 32 threads. The warp is processed by the SM concurrently. 
For one warp instruction to be executed requires 4 SM clock cycles. 
SM's execute multiple warp instructions. The warps instructions are 
prioritized and scheduled to minimize overhead. 

Volta and more recent GPU architectures use independent thread 
scheduling. In addition, each thread can access memory within a 
unified virtual address space. Threads must synchronize with other 
threads using execution barriers, synchronization primitives and 
Cooperative Groups to utilize unified memory.

SYCL has a similar execution hierarchy consisting of kernels. 
The kernel is broken down into work-items. Each work-item concurrently
executes an instance of the kernel on a piece of memory. Work-items 
can be combined into work-groups that have designated shared memory.
Work-groups can synchronize their work-items with work-group barriers.

There are some equivalences between CUDA and SYCL execution models. 
For example, CUDA's stream multiprocessor is equal to a SYCL compute 
unit. CUDA's grid is similar to SYCL's nd_range as it is the highest 
level grouping of threads, not including the whole kernel. Both 
nd_range and grid can segment the groups of threads into one, two, or 
three dimensions. SYCL sub-groups roughly map to
cooperative groups `thread_block_tile` as it allows for the
work-group/thread block to be further subdivided into concurrent threads.
Likewise, thread blocks map directly to work-groups, and a
single thread is a SYCL work-item.

CUDA primarily synchronizes the threads through two functions,
`cudaStreamSynchronize()` and `__syncthreads()`. 
`cudaStreamSynchronize()` blocks work from being performed until all 
threads on the device has been completed. `__syncthreads()` waits for 
all threads within a thread block to reach the same point. So 
`cudaStreamSynchronize()` is similar to queue.wait(), buffer 
destruction, and other host-device synchronization events within SYCL. 
`__syncthreads()` synchronizes the threads within a thread block which
is analogous to the work-group barrier.

CUDA's warp concept has no SYCL equivalent. If a user were to write 
warp aware code it would be non-generic SYCL code and specific to the 
CUDA backend.

CUDA allows for more detailed thread and memory management through 
Cooperative Groups. Cooperative Groups allow for synchronizing at the 
grid level and organizing subgroups in sizes smaller than a warp. 
Cooperative Groups do not have an equivalent within SYCL 2020 and are 
not yet supported.

[[table.cuda.CUDA_features_to_SYCL]]
.CUDA execution features with their corresponding SYCL features
[width="100%",options="header",cols="50%,50%"]
|====
| [code]#SYCL#                                                       | [code]#CUDA#
| [code]#Compute unit#                                               | [code]#Streaming multiprocessor#
| [code]#nd_range#                                                   | [code]#grid#
| [code]#work-group#                                                 | [code]#Thread block#
| [code]#sub-group#                                                  | [code]#thread_block_tile#
| [code]#work-item#                                                  | [code]#Thread#
| [code]#SYCL nd_item synchronization#                               | [code]#cudaStreamSynchronize#
| [code]#work-group barrier#                                         | [code]#__syncthread#
|====

[[sec::programming_interface]]
== Programming Interface

[[sub:cuda:application_interoperability]]
=== Application Interoperability

[[table.cuda.appinterop.nativeobjects]]
.Types of native backend objects application interoperability
[width="100%",options="header",cols="20%,20%,20%,40%"]
|====
| [code]#SyclType#                                                   | [code]#backend_input_t<backend::cuda, SyclType># | [code]#backend_return_t<backend::cuda, SyclType># | Description
| [code]#buffer# |   |   |
| [code]#context#                                                   |   |   |
| [code]#device#                                                    |   |   |
| [code]#device_image<State>#                                       |   |   |
| [code]#event#                                                     |   |   |
| [code]#kernel#                                                    |   |   |
| [code]#kernel_bundle<State>#                                      |   |   |
| [code]#platform#                                                  |   |   |
| [code]#queue#                                                     |   |   |
| [code]#sampled_image<Dims, AllocatorT>#                           |   |   |
| [code]#unsampled_image<Dims, AllocatorT>#                         |   |   |
|====

[[table.cuda.appinterop.ownership]]
.Ownership behavior of native backend objects.
[width="100%",options="header",cols="40%,60%"]
|====
| SYCL Object                                                       | Destructor behaviour
| [code]#buffer# |   
| [code]#context#                                                   |   
| [code]#device#                                                    |  
| [code]#device_image<State>#                                       |   
| [code]#event#                                                     |  
| [code]#kernel#                                                    |   
| [code]#kernel_bundle<State>#                                      |   
| [code]#platform#                                                  |  
| [code]#queue#                                                     |   
| [code]#sampled_image<Dims, AllocatorT>#                           |   
| [code]#unsampled_image<Dims, AllocatorT>#                         | 
|====

[[table.cuda.appinterop.make_interop_APIs]]
.[code]#make_*# Interoperability APIs for native backend objects.
[width="100%",options="header",cols="40%,60%"]
|====
| CUDA interoperability function                                    |  Description
| [code]#template<backend Backend> +
platform + 
make_platform(const backend_input_t<Backend, platform> &backendObject);# 
        |

| [code]#template<backend Backend> +
device +
make_device(const backend_input_t<Backend, device> &backendObject);# 
        |

| [code]#template<backend Backend> +
context +
make_context(const backend_input_t<Backend, context> &backendObject,
                     const async_handler asyncHandler = {});# 
        |

| [code]#template<backend Backend> +
queue +
make_queue(const backend_input_t<Backend, queue> &backendObject,
                 const context &targetContext,
                 const async_handler asyncHandler = {});# 
        |

| [code]#template<backend Backend> +
event +
make_event(const backend_input_t<Backend, event> &backendObject,
                 const context &targetContext);# 
        |

| [code]#template <backend Backend, typename T, int dimensions = 1,
          typename AllocatorT = buffer_allocator<std::remove_const_t<T>>> +
buffer<T, dimensions, AllocatorT> +
make_buffer(const backend_input_t<Backend, buffer<T, dimensions, AllocatorT>>
                &backendObject,
            const context &targetContext, event availableEvent);# 
        |

| [code]#template <backend Backend, typename T, int dimensions = 1,
          typename AllocatorT = buffer_allocator<std::remove_const_t<T>>> +
buffer<T, dimensions, AllocatorT> +
make_buffer(const backend_input_t<Backend, buffer<T, dimensions, AllocatorT>>
                &backendObject,
            const context &targetContext);# 
        |

| [code]#template <backend Backend, int dimensions = 1,
          typename AllocatorT = sycl::image_allocator> +
sampled_image<dimensions, AllocatorT> + 
make_sampled_image(
    const backend_input_t<Backend, sampled_image<dimensions, AllocatorT>>
        &backendObject,
    const context &targetContext, image_sampler imageSampler,
    event availableEvent);# 
        |

| [code]#template <backend Backend, int dimensions = 1,
          typename AllocatorT = sycl::image_allocator> +
sampled_image<dimensions, AllocatorT> +
make_sampled_image(
    const backend_input_t<Backend, sampled_image<dimensions, AllocatorT>>
        &backendObject,
    const context &targetContext, image_sampler imageSampler);# 
        |

| [code]#template <backend Backend, int dimensions = 1,
          typename AllocatorT = sycl::image_allocator> +
unsampled_image<dimensions, AllocatorT> +
make_unsampled_image(
    const backend_input_t<Backend, unsampled_image<dimensions, AllocatorT>>
        &backendObject,
    const context &targetContext, event availableEvent);# 
        |

| [code]#template <backend Backend, int dimensions = 1,
          typename AllocatorT = sycl::image_allocator> +
unsampled_image<dimensions, AllocatorT> + 
make_unsampled_image(
    const backend_input_t<Backend, unsampled_image<dimensions, AllocatorT>>
        &backendObject,
    const context &targetContext);# 
        |

| [code]#template<backend Backend, bundle_state State> +
kernel_bundle<State> +
make_kernel_bundle(
    const backend_input_t<Backend, kernel_bundle<State>> &backendObject,
    const context &targetContext);# 
        |

| [code]#template<backend Backend> +
kernel +
make_kernel(const backend_input_t<Backend, kernel> &backendObject,
                   const context &targetContext);# 
        |
|====

[[table.cuda.appinterop.make_interop_APIs]]
.[code]#get_native# Interoperability APIs for native backend objects.
[width="100%",options="header",cols="40%,60%"]
|====
| CUDA interoperability function                                    |  Description
| [code]#template<backend Backend, class T> +
backend_return_t<Backend, T> +
get_native(const T &syclObject);# 
        |
|====


[[sub:cuda:kernel_function_interoperability]]
=== Kernel Function Interoperability

[[table.cuda.appinterop.nativeobjects]]
.Types of native backend objects kernel function interoperability
[width="100%",options="header",cols="20%,20%,20%,40%"]
|====
| [code]#SyclType#                                                   | [code]#backend_input_t<backend::cuda, SyclType># | [code]#backend_return_t<backend::cuda, SyclType># | Description
| [code]#accessor<T, Dims, Mode, target::device>#                    |   |   |
| [code]#accessor<T, Dims, Mode, target::constant_buffer>#           |   |   |
| [code]#accessor<T, Dims, Mode, target::local>#                     |   |   |
| [code]#local_accessor<T, Dims>#                                    |   |   |
| [code]#sampled_image_accessor<T, 1, Mode, image_target::device>#   |   |   |
| [code]#sampled_image_accessor<T, 2, Mode, image_target::device>#   |   |   |
| [code]#sampled_image_accessor<T, 3, Mode, image_target::device>#   |   |   |
| [code]#unsampled_image_accessor<T, 1, Mode, image_target::device># |   |   |
| [code]#unsampled_image_accessor<T, 2, Mode, image_target::device># |   |   |
| [code]#unsampled_image_accessor<T, 3, Mode, image_target::device># |   |   |
| [code]#stream#                                                     |   |   |
| [code]#device_event#                                               |   |   |
|====


[[sec:non_core_features_and_extensions]]
== Non-core features and extensions

Additional CUDA features are available depending upon the devices compute capability.
SYCL can support these optional CUDA features with extensions.
Unlike OpenCL, CUDA needs to know if the extension is available at compile time. 
As a result there are no valid CUDA extensions which can be passed to `has_extension`.

As the extension must be known at runtime CUDA extensions are best implemented 
using feature test macros. The test macro format is 
SYCL_EXT_<vendorstring>_<featurename>. For CUDA extensions this format translates 
to SYCL_EXT_NVIDIA_<featurename>. Similarly, the format for the naming of extension 
classes and enumerations should be ext_<vendorstring>_<featurename>. Which in this context
becomes ext_NVIDIA_<featurename>. Given the necessity to know the extension at 
compile-time, the usage of extension macros should be the primary method of determining 
if the extension is available in the SYCL implementation not. 
A list of non-core CUDA features which have SYCL support is below.
Non-core CUDA features for require a compute capability of greater than 5.

TODO: The table below shows a proposal for SYCL supported CUDA extensions.
The table should be developed with other members of the SYCL community.

[[table.extensionsupport]]
.SYCL support for CUDA 11.3 extensions
[width="100%",options="header",cols="35%,35%,15%, 15"]
|====
| SYCL Aspect              | CUDA Extension                                        | Core SYCL API | Required Compute Capability 
| [code]#aspect::fp16#     | [code]#16-bit floating point#                         | Yes           | 5.3 or greater
| -                        | [code]#Tensor Cores#                                  | No            | 7 or greater
| -                        | [code]#Atomic floating-point operations#              | No            | 6 or greater
|====

=== Aspects
Aspects are used to query what features and attributes a device has. Some aspects such as `fp16`
are non-core CUDA features. Therefore, the runtime must be able to determine what aspects CUDA 
devices have. This can be performed by querying `cudaDeviceProp::major` and `cudaDeviceProp::minor`
to find out the compute capability. The compute capability indicates what extensions are
available to the device, and therefore what aspects are available.

[[sec:cuda:extension-fp16]]
=== Half precision floating-point

The half scalar data type: [code]#half# and the half vector data types:
[code]#half1#, [code]#half2#, [code]#half3#,
[code]#half4#, [code]#half8# and [code]#half16# must be
available at compile-time. However a kernel using these types is only
supported on devices that have [code]#aspect::fp16#, i.e. compute capability
5.3 or greater.

[[sub:cuda:extensions]]
=== Extensions

[[sub:cuda:error_handling]]
=== Error Handling

// %%%%%%%%%%%%%%%%%%%%%%%%%%%% end cuda_backend %%%%%%%%%%%%%%%%%%%%%%%%%%%%
